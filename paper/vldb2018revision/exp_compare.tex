\changed{
%\subsection{Compare with Other OLTP Kernels}
\subsection{Integration with Cicada}
\label{sec:OtherOLTP}
We implemented our idea of transaction batching and reordering also in the Cicada open-source OLTP system~\cite{lim2017cicada}. In Cicada, every thread executes its transactions independently, and there is no centralized thread for transaction validation and storage access. We integrated our techniques with its architecture by batching and reordering the transactions in a preprocessing step as described in Section~\ref{subsec:validator_reordering:policy}. 

We compare our strategy ($rc$) with a number of the-state-of-the-art OLTP kernels, including Cicada ($cicada$)~\cite{lim2017cicada}, Silo ($silo$)~\cite{tu2013speedy}, ERMIA ($ermia$)~\cite{kim2016ermia}, and TicToc ($tictoc$)~\cite{yu2016tictoc}, using the same YCSB benchmark configuration used 
in the original Cicada paper~\cite{lim2017cicada}. Each transaction consists of 16 keys with a 50\% read ratio.

We run the experiment on a machine with two Intel® Xeon® Processor E5-2690 v4 CPUs (each with 14 physical cores) and 256GiB of DRAM. The experiments pin threads to different cores and use NUMA-aware memory to allocate hugepages. 

Figure~\ref{fig:oltp_kernel:tps_z0.99} shows the throughput of different systems with a write-intensive, highly-skewed workload varying the number of threads. Figures~\ref{fig:oltp_kernel:tps_t28} and~\ref{fig:oltp_kernel:latency} show the throughput and percentile latency with the maximal number of threads (28) under workloads at different degrees of skewness.}

\changed{
As the degree of data contention increases, either with more threads or more skewed data distribution, reordering outperforms the three other kernels resulting in higher throughput and lower tail latencies. With 56 threads, reordering improves throughput by a factor of 1.8, reduces aborts by 1/3, and it reduces the 99.9\% 
latency by 7/8!
}

\changed{
In this experiment, we configure our strategy to reorder a batch size of $4\times num\_threads$ and schedule $4$ transactions to each thread within a batch after reordering. The reordering is up to 5x faster than processing a transaction, which can be further parallelized and preprocessed in a separate component, e.g., at web servers.
}

\subsection{Implementation on DBMS-X}
\label{subsec:experiment:compare}

In our final experiment, we implemented the idea of transaction batching and reordering on top of a commercial DBMS-X. DBMS-X is a high performance OLTP engine using optimistic concurrency control. Upon receiving transactions, it processes transactions concurrently with a first-come-first-served order.

In many applications and services, clients submit transactions to databases via a middle-layer, such as a web server. This server consolidates requests from many clients, processes the requests, reroutes the requests to different database servers, and responds to the clients. The web server often batches transactions from different clients to improve throughput and resource efficiency. 

We incorporate the idea of transaction batching and reordering to DBMS-X at the client side.
We implemented validator reordering for the transactions batched at the middle-layer -- before submitting the batched transactions to the database server. Since the transactions haven't started executing and their read timestamps are not yet available, we conservatively assume that all the transactions in a batch read from the same snapshot of the database. We analyze the potential conflicts between the transactions, and then we maximize the number of transaction commits with our reordering algorithm, using a policy considering both how many dependencies a transaction involves and how long it has been waiting. The transactions excluded from the batch, together with future incoming transactions, will be included in the next batch for reordering.

We use SmallBank benchmark of Zipfian skew 0.9 as a high-data-contention scenario. We compare transaction batching and reordering (\emph{BatchReorder}) with two baselines: no batching (\emph{NoBatch}) and batch without reordering (\emph{Batch}). In \emph{NoBatch}, we submit transactions to DBMS-X one at a time. In \emph{Batch} and \emph{BatchReorder}, we batch transactions before sending them to DBMS-X. We choose a batch size of 50, which gives reasonable throughput for this workload.

Figure~\ref{fig:hekaton:tps}, ~\ref{fig:hekaton:abort}, and ~\ref{fig:hekaton:latency} show the throughput, the latency, and the abort rate when we increase the number of database connections. 

\changed{We issue transactions to DBMS-X using JDBC. A connection to DBMS-X can send transaction statements individually or in batch with JDBC calls, where the call returns after all the transactions are processed (either commit or abort). As DBMS-X receives a batch of transaction statements from a connection, it can execute them concurrently. Batching transaction statements reduces the overhead of communication and can increase the concurrent level of DBMS-X.}
	
When we do not batch the transactions, the number of concurrent transactions is small and the throughput is low. As a result of low concurrency level, the chance of conflicting is slim. Thus, both abort rate and latency are low. As we send transactions in batches, the throughput increases dramatically. 

However, as the load continues to increase, the system runs into data contention. This further leads to resource contention due to restarts. Thus, the abort rate and the latency rise significantly. When we both batch and reorder transactions, the performance improves in all metrics: peak throughput increased by 1.25x, throughput increased by up to 3.1x, latency reduced by up to 66\%, and abort rate dropped by up to 62\%. In addition, the system performance degrades much more gracefully with increasing load.
