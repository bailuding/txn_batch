\documentclass{article}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{colonequals}
\usepackage[in]{fullpage}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{subcaption}
%\usepackage{stmaryrd}
\usepackage{indentfirst}
\usepackage{xcolor}

\newcommand{\eat}[1]{}
\newcommand{\eval}[1]{[\![#1]\!]~}
\newcommand{\tuple}[1]{\langle #1 \rangle}
\newcommand{\nil}{\texttt{nil}}
\newcommand{\concat}{\mathrel{\hbox{\scriptsize+}\!\hbox{\scriptsize+}}}

\newcommand{\authorcomment}[2]{{\color{red} !#1: #2}}
\newcommand{\bailu}{\authorcomment{Bailu}}

\long\def\cut#1{{}}

\ifdefined\submit
\newcommand{\todo}[1]{}
\newcommand{\changed}[1]{#1}
\long\def\tocut#1{}
\else
\newcommand{\todo}[1]{\textcolor{red}{\bf [TODO!: #1]}}
\newcommand{\changed}[1]{{\color{blue}#1}}
\newcommand{\tocut}[1]{\textcolor{red}{\it \st{#1}}}
\fi

\begin{document}
\title{Response to Reviews for ``\emph{Improving OCC Performance Through Transaction Batching and Operation Reordering}''}
\author{Bailu Ding, Lucja Kot, Johannes Gehrke}
\date{}
\maketitle

We would like to thank the reviewers and the meta-reviewer for their insightful comments and suggestions. Below is a summary of the changes we made in response to these comments. We have fixed typos and omit such comments in the response. Changes are marked in blue in both the response and the revised submission.

\section{Changes based on meta-reviewer comments}

We first summarize and address the major comments from the reviews.

\begin{itemize}
	\item[(R1)] \emph{Expand the experimental evaluation to include a comparison with other high-performance OLTP engines that use OCC.}
\end{itemize}

\changed{
We integrated our techniques to Cicada and compared its performance with Cicada, Silo, TicToc, and ERMIA. Our techniques achieved at least 2x throughput on write-intensive YCSB benchmark compared with these state-of-the-art OLTP kernels.

The experiment details are described in Section 6.7.
}

\begin{itemize}
	\item[(R2)] \emph{Fix typos, unclear parts, and expand discussion per the reviewers requests.}
\end{itemize}

\changed{
	We fixed all the typos mentioned by the reviewers. (TODO)
}

\section{Changes based on individual reviewer comments}

In this section we explain how we addressed individual reviewer comments which are not directly subsumed by a metareviewer comment. We have addressed comments on clarifications of the writing, and omitted similar comments that can be addressed by previous responses.

\subsection{Reviewer 1}

\begin{itemize}
\item[(R1.1)] \emph{Add an experiment to show the performance of the proposed techniques in systems where the clients do not send their next transaction until their previous one is finished.}
\end{itemize}
\changed{
	We integrated our techniques to Cicada, where each thread processes a transaction synchronously. Please see R1 for details.
}

\begin{itemize}
\item[(R1.2)] \emph{DBMS-X Add a more detailed description regarding the DBMS-X experiment.}
\end{itemize}

\changed{
	Our prototype issues transactions to DBMS-X via JDBC, where it can submit transaction statements individually or in batch. Either way, the connection will block until all the transactions get processed, i.e., commit or abort. As DBMS-X receives a batch of transaction statements from a connection, it can execute them concurrently. Batching transaction statements reduces the overhead of communication and can increase the concurrent level of DBMS-X as it receives more active transactions.
	
	We added a more detailed description of why batching helps DBMS-X in Section 6.7.
}

\begin{itemize}
\item[(R1.3)] \emph{Improve the legends of the figures.}
\end{itemize}

\changed{
	We updated the legend to replace unnecessary abbreviations, e.g., bch -> batch.
}

\begin{itemize}
\item[(R1.4)] \emph{Add an experiment to demonstrate to generality of the sort-based algorithm by using more policies or benchmarks.}
\end{itemize}

\changed{
We agree that the policies should be customized based on the objective of transaction processing system and its architecture.

We further extended our sort-based framework to incorporate a thread-aware policy for an important class of multi-threaded OLTP architecture, where each transaction is processed independently and synchronously by a dedicated thread.

We implemented the policy on top of Cicada and please see Section 6.7 for the evaluation details.
}

\subsection{Reviewer 2}

\begin{itemize}
\item[(R2.1)] \emph{Add a discussion which explains what constraints on the system make the system best suited for integration with an in-memory versioned key-value store and what would need to change to be integrated with other stores.}
\end{itemize}

\changed{
	We retreated this statement from the introduction. In our experiments, we integrated our techniques to two systems that are not based on key-value stores, i.e., Cicada (supporting hash index and B+ tree index) and DBMS-X (using BW-tree). The result shows that a variety of storage and transaction processing architectures can benefit from our techniques.
}

\begin{itemize}
\item[(R2.2)] \emph{There are many moving parts in the experiments so it is difficult to picture the tradeoffs holistically. I suggest that either the results are represented in a formula or represented in a table showing the tradeoffs.}
\end{itemize}

\todo{Add to appendix}
\changed{
	We added a table to summarize the trade-off of different parameters and policies in the Appendix.
}

\subsection{Reviewer 3}

\begin{itemize}
\item[(R3.1)] \emph{The papers contains a bag of ideas/optimizations, arguably unrelated,
	based on known techniques, so the overall contributions and novelty is
	limited.}
\end{itemize}

\todo{Better wording needed}
\changed{
	\begin{itemize}
		\item Our contribution is a general framework of batching and reordering for OCC based OLTP systems.
		\item The framework uses a sort-based policy to customize for different transaction processing performance objectives and system architectures.
	\end{itemize}
}

\begin{itemize}
\item[(R3.2)] \emph{Although the evaluation is comprehensive and detailed, but the authors
	only presented a micro benchmark, a self-comparison without considering
	other state-of-the-art approaches. Here are few important related CCs
	(related work discussion can also take into these approaches as well)}
\end{itemize}

\changed{
	Please see R1 for details.
}

\begin{itemize}
\item[(R3.3)] \emph{The authors provide a black box comparison of their approach in DB-X,
	but the basis of the comparison is unclear (also not sure what does
	"Good Throughput/Transaction" mean). Although the effort is appreciated,
	it would be much better to compare with relevant, disclosed existing
	algorithm, or at the very least, the concurrency of the model of DB-X is
	carefully explained. A black box graph adds no value.}
\end{itemize}

\changed{
	"Good Throughput" means the number of committed transactions per second, since some literature refers "Throughput" as the number of transactions processed per second, regardless either they commit or abort. We changed to term to "throughput" and clarified its semantics here in Section 6.1.
	
	We added an experiment to compare with other state-of-the-art OLTP kernels. Please see R1 for details.
	
	DBMS-X is Hekaton in Microsoft SQL Server. Due to confidentiality, we could not disclose the name of the system or too much details of the system to make it identifiable. 
}


\begin{itemize}
\item[(R3.4)] \emph{It is unclear why the authors choose to have 300 active transactions,
	which would imply the need for having 300 physical threads. I further
	suppose, the authors considering in-memory implementation given all OLTP
	DB can fit entirely in memory today. If in fact, the number of active
	transactions is larger than threads, then there will be many context switches,
	and frankly, the whole setting would be questionable, which could also
	explain why the overall throughput is never exceeded half-million
	transactions/second.}
\end{itemize}

\changed{
	As described in Section 6.1, our prototype for micro benchmark experiment is implemented with asynchronous transaction processing architecture. Each thread multiplexes multiple transactions simultaneously to mask the latency of transaction workflow execution, IO accesses, and network communication. Thus, the number of threads can be much less than the number of transactions. This architecture has loosely coupled components and can scale storage and compute independently, which is especially applicable for elastic transaction processing in the cloud.

	We also integrated our techniques to Cicada, where each thread processes a transaction synchronously and independently. In our integration with Cicada, each thread is pinned to a dedicated physical CPU core and the number of active transactions never exceeds the number of cores. We achieved close to 2 millions transactions per second with highly skewed, write-intensive workload, which is at least 2x the throughput of the state-of-the-art OLTP kernels.  
}

\begin{itemize}
\item[(R3.5)] \emph{In some graphs, x-axis label is cut off.}
\end{itemize}

\changed{
	We fixed the truncated labels and ticks.
}

\end{document}