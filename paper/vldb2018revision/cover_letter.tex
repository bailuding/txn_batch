\documentclass{article}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{colonequals}
\usepackage[in]{fullpage}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{subcaption}
%\usepackage{stmaryrd}
\usepackage{indentfirst}
\usepackage{xcolor}

\newcommand{\eat}[1]{}
\newcommand{\eval}[1]{[\![#1]\!]~}
\newcommand{\tuple}[1]{\langle #1 \rangle}
\newcommand{\nil}{\texttt{nil}}
\newcommand{\concat}{\mathrel{\hbox{\scriptsize+}\!\hbox{\scriptsize+}}}

\newcommand{\authorcomment}[2]{{\color{red} !#1: #2}}
\newcommand{\bailu}{\authorcomment{Bailu}}

\long\def\cut#1{{}}

\ifdefined\submit
\newcommand{\todo}[1]{}
\newcommand{\changed}[1]{#1}
\long\def\tocut#1{}
\else
\newcommand{\todo}[1]{\textcolor{red}{\bf [TODO!: #1]}}
\newcommand{\changed}[1]{{\color{blue}#1}}
\newcommand{\tocut}[1]{\textcolor{red}{\it \st{#1}}}
\fi

\begin{document}
\title{Response to Reviews for ``\emph{Improving OCC Performance Through Transaction Batching and Operation Reordering}''}
\author{Bailu Ding, Lucja Kot, Johannes Gehrke}
\date{}
\maketitle

We would like to thank the reviewers and the meta-reviewer for their insightful comments and suggestions. Below is a summary of the changes we made in response to these comments. Changes are marked in blue in both the response and in the revised submission.

\section{Changes based on meta-reviewer comments}

We first summarize and address the major comments from the reviews.

\begin{itemize}
	\item[(R1)] \emph{Expand the experimental evaluation to include a comparison with other high-performance OLTP engines that use OCC.}
\end{itemize}

\changed{
We integrated our techniques into the Cicada open-source OLTP kernel, and we compared its performance with the state-of-the-art in-memory OLTP kernels such as Cicada, Silo, TicToc, and ERMIA. Our techniques improve throughput by at least a factor of two on the write-intensive YCSB benchmark compared with these systems.

We would also like to note that we used exactly the same parameter settings and the same setup as in the original Cicada paper. The new results are described in Section 5.6.
}

\begin{itemize}
	\item[(R2)] \emph{Fix typos, unclear parts, and expand discussion per the reviewers requests.}
\end{itemize}

\changed{
	We fixed all the typos mentioned by the reviewers.
}

\section{Changes based on individual reviewers' comments}

In this section we explain how we addressed the comments of individual reviewers in case the comments are not directly subsumed by a meta-reviewer comment. We have addressed comments on clarifications of the writing, and omitted similar comments that can be addressed by previous responses.

\subsection{Reviewer 1}

\begin{itemize}
\item[(R1.1)] \emph{Add an experiment to show the performance of the proposed techniques in systems where the clients do not send their next transaction until their previous one is finished.}
\end{itemize}
\changed{
	We integrated our techniques to Cicada, where each thread processes a transaction synchronously. Please see Major Comment R1 for details.
}

\begin{itemize}
\item[(R1.2)] \emph{Add a more detailed description regarding the DBMS-X experiment.}
\end{itemize}

\changed{
	Our prototype issues transactions to DBMS-X via JDBC, where it can submit transaction statements individually or in batch. Either way, the connection will block until all the transactions get processed, i.e., commit or abort. As DBMS-X receives a batch of transaction statements from a connection, it can execute them concurrently. Batching transaction statements reduces the overhead of communication and can increase the concurrency level of DBMS-X as it receives more active transactions.
	
	We added a more detailed description of why batching helps DBMS-X in Section 5.7.
}

\begin{itemize}
\item[(R1.3)] \emph{Improve the legends of the figures.}
\end{itemize}

\changed{
	We updated the legends to replace unnecessary abbreviations, e.g., $bch \to batch$.
}

\begin{itemize}
\item[(R1.4)] \emph{Add an experiment to demonstrate the generality of the sort-based algorithm by using more policies or benchmarks.}
\end{itemize}

\changed{
We agree that the policies should be customized based on the objective of transaction processing system and its architecture.

We also appreciate this comment, and we say that full generality will be hard to show. We extended our sort-based framework to incorporate a thread-aware policy for an important class of multi-threaded OLTP architecture, where each transaction is processed independently and synchronously by a dedicated thread. We thus believe that our sort-based approach is rather general, and that we have a shown a wide variety of both scenarios and also different types of systems where this approach has merit. 

We implemented the policy on top of Cicada; please see Section 5.6 for the evaluation details.
}

\subsection{Reviewer 2}

\begin{itemize}
\item[(R2.1)] \emph{Add a discussion which explains what constraints on the system make the system best suited for integration with an in-memory versioned key-value store and what would need to change to be integrated with other stores.}
\end{itemize}

\changed{
	We retreated this statement from the introduction. In our experiments, we integrated our techniques to two systems that are not based on key-value stores, i.e., Cicada (supporting hash index and B+ tree index) and DBMS-X (using the BW-tree). The result shows that a variety of storage and transaction processing architectures can benefit from our techniques.
}

\begin{itemize}
\item[(R2.2)] \emph{There are many moving parts in the experiments so it is difficult to picture the tradeoffs holistically. I suggest that either the results are represented in a formula or represented in a table showing the tradeoffs.}
\end{itemize}

\todo{Add to appendix}
\changed{
	We added a table to summarize the trade-off of different parameters and policies in Table 1.
}

\subsection{Reviewer 3}

\begin{itemize}
\item[(R3.1)] \emph{The papers contains a bag of ideas/optimizations, arguably unrelated,
	based on known techniques, so the overall contributions and novelty is
	limited.}
\end{itemize}

\todo{Better wording needed}
\changed{
	We believe that our contribution is a rather general framework for batching and associated policy-based reordering of transactions for OLTP systems. As we have shown, our sort-based policy can be customized for different transaction processing performance objectives and for different system architectures. As thus, we believe that this combination is novel and will have practical impact.
}

\begin{itemize}
\item[(R3.2)] \emph{Although the evaluation is comprehensive and detailed, but the authors
	only presented a micro benchmark, a self-comparison without considering
	other state-of-the-art approaches. Here are few important related CCs
	(related work discussion can also take into these approaches as well)}
\end{itemize}

\changed{
	 We integrated our techniques into the Cicada open-source OLTP kernel, and we compared its performance with the state-of-the-art in-memory OLTP kernels such as Cicada, Silo, TicToc, and ERMIA. Please see Comment R1 for more details.
}

\begin{itemize}
\item[(R3.3)] \emph{The authors provide a black box comparison of their approach in DB-X,
	but the basis of the comparison is unclear (also not sure what does
	"Good Throughput/Transaction" mean). Although the effort is appreciated,
	it would be much better to compare with relevant, disclosed existing
	algorithm, or at the very least, the concurrency of the model of DB-X is
	carefully explained. A black box graph adds no value.}
\end{itemize}

\changed{
	"Good Throughput" means the number of committed transactions per second, since some literature refers to "Throughput" as the number of transactions processed per second, no matter they commit or abort. We changed to term to "throughput" and clarified its semantics in Section 6.1.
	
	
	We cannot disclose the name of DB-X for confidentiality reasons, but it uses a very specific OCC protocol. However, our techniques are not at all customized for its protocol. 	We also added a whole new section to compare with other state-of-the-art OLTP kernels to create a comparison beyond DB-X. Please see Comment R1 for details.
	
}


\begin{itemize}
\item[(R3.4)] \emph{It is unclear why the authors choose to have 300 active transactions,
	which would imply the need for having 300 physical threads. I further
	suppose, the authors considering in-memory implementation given all OLTP
	DB can fit entirely in memory today. If in fact, the number of active
	transactions is larger than threads, then there will be many context switches,
	and frankly, the whole setting would be questionable, which could also
	explain why the overall throughput is never exceeded half-million
	transactions/second.}
\end{itemize}

\changed{
	As described in Section 6.1, our prototype for the micro-benchmark experiment is implemented with asynchronous transaction processing architecture. Each thread multiplexes multiple transactions simultaneously to mask the latency of transaction workflow execution, IO accesses, and network communication. Thus, the number of threads can be much less than the number of active transactions. This architecture has loosely coupled components and can scale storage and compute independently, which is especially applicable for elastic transaction processing in the cloud.

	We also integrated our techniques to Cicada, where each thread processes a transaction synchronously and independently. In the integrated system, each thread is pinned to a dedicated physical CPU core and the number of active transactions never exceeds the number of cores. We achieved close to 2 millions transactions per second under highly skewed, write-intensive workload, which is at least 2x the throughput compared with the state-of-the-art OLTP kernels.  
}

\begin{itemize}
\item[(R3.5)] \emph{In some graphs, x-axis label is cut off.}
\end{itemize}

\changed{
	We fixed the truncated labels and ticks.
}

\end{document}