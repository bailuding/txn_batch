\subsection{Implementation on DBMS-X}
\label{subsec:experiment:compare}

In our final experiment, we implemented the idea of transaction batching and reordering on the client side of a commercial DBMS-X. DBMS-X is a high performance OLTP engine using optimistic concurrency control. Upon receiving transactions, it processes transactions concurrently with a first-come-first-server order. We implement transaction batching and reordering at the client side of DBMS-X as a proof-of-concept.

In many applications and services, clients submit transactions to databases via a middle-layer, such as a web server. This web server consolidates requests from potentially a large number of clients, processes the requests, reroutes the requests to different database servers, and responds to the clients. The web server often batches transactions from different clients to improve throughput and resource efficiency. 

As a prototype, we implemented validator reordering for the transactions batched at the web server. Since the transactions haven't started executing and their read timestamps are not available, we conservatively assume all the transactions in the batch read from the same snapshot of the database. We analyze the potential conflicts between the transactions, and then we maximize the number of commits of transactions with our reordering algorithm, using a policy considering both how many dependencies a transaction involves and how long it has been waiting. For the transactions excluded from the batch, they will be included in the next batch for reordering, together with future incoming transactions.

We use SmallBank benchmark of Zipfian skew 0.9 as a highly contended scenario. We compare transaction batching and reordering (\emph{BatchReorder}) with two other baselines: no batching (\emph{NoBatch}) and batch without reordering (\emph{Batch}). In \emph{NoBatch}, we transactions to DBMS-X one at a time. In \emph{Batch} and \emph{BatchReorder}, we batch the transactions before sending the transactions to DBMS-X. We choose a batch size of 50, which is a throughput-wise reasonable batch size for this workload.

Figure~\ref{fig:hekaton:tps}, ~\ref{fig:hekaton:abort}, and ~\ref{fig:hekaton:latency} show the throughput, latency, and abort rate when we increase the number of database connections. When we do not batch the transactions, the number of concurrent transactions is small and the throughput is low. As a result of low concurrency level, the chance of conflicting is small. Thus, both abort rate and latency are low. As we send transactions in batches, the throughput increases dramatically. However, as the load continues to increase, the system runs into data contention, which leads to resource contention due to restarts. The abort rate and latency rise significantly. When we both batch and reorder the transactions, the performance improves in all metrics: higher peak throughput by 1.25x, higher throughput by up to 3.1x, lower latency by up to 66\%, and abort rate by up to 62\%. In addition, the system performance degrades much more gracefully when the system load continues to increase.
