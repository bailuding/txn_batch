\section{Batching and Reordering: Concepts}\label{sec:overview}

%In this section, we introduce our novel ideas of batching at the storage and the validator in an OCC system, and we formulate the resulting algorithmic problems. 

\emph{Batching} involves buffering a number of operations as they arrive at some component of the system and processing them as a group. 
Given a batch, we run a lightweight algorithm that analyzes the batch and then reorders the operations in the batch in order to reduce aborts. We will introduce two types of reordering opportunities: Storage batching and validator batching.

We first make a conceptual distinction between two types of transaction aborts: intra-batch and inter-batch aborts. Assume transaction $T$ abort due to its conflict with $T'$. If $T$ and $T'$ are in the same batch, we call the resulting abort of $T'$ an \emph{intra-batch abort}; otherwise, we call it an \emph{inter-batch abort}.
%Batching transactions that access the same objects together reduces inter-batch aborts. 
%There has been a lot of research on data clustering either online or offline~\cite{elmore2015squall, pavlo2012skew}, especially for fine-grained data partitioning. 
In this work, we focus on managing intra-batch aborts by strategically reordering the batched requests at storage and validator. 
Reducing inter-batch aborts is complementary to our effort. 

Our approach is agnostic to isolation levels as long as the reordering respects the corresponding definitions of conflicts for 
write-read, write-write, and read-write dependencies. In the remainder of this paper, we describe 
how to batch and reorder for serializability, but our techniques can be adapted accordingly to other isolation levels, 
such as snapshot isolation.

\eat{For example, in snapshot isolation, if the storage is multi-versioned and offers a consistent snapshot to reads, no reordering is needed. At the validator, two transactions in the same batch conflict with each other if they write to the same
object. Thus, we create a dependency graph based on write-write conflicts, and the other parts of the
protocol stay the same. For example, let $T_1$, $T_2$, and $T_3$ be in the same batch, where $T_1$ writes to
$X$, $T_2$ writes to $Y$, and $T_3$ writes to $X$ and $Y$. Committing $T_3$ will lead to the aborts of $T_1$ and $T_2$. But if
we choose to abort $T_3$, $T_1$ and $T_2$ can both commit. }

\subsection{Reordering at the Storage}\label{subsec:storage_reordering}
If a transaction reads a stale version of an object from the storage layer, it is bound to 
abort at the validator as it conflicts with the update from a committed transaction. 
Thus, applying updates at the storage layer as early as possible can 
reduce the chance of aborts for incoming transactions.

We implement this idea as follows. We buffer a number of read and write requests from transactions into batches. 
As a batch of requests arrives at the storage layer, for each object, we apply the highest-version write request on that object. It is safe to discard all other writes on that object as explained in Section~\ref{sec:background}.\footnote{Recall that we assume full serializability; this may be different for snapshot isolation.} 
Next, we handle all the read requests for the same object. This strategy works very well for reducing intra-batch aborts, 
as it ensures that all available writes by committed transactions are applied to all objects before we handle any read requests on these objects. 

\subsection{Reordering at the Validator}\label{subsec:validator_reordering}

In validator batching, we buffer transaction validation requests at the validator as they arrive. Once a batch has been collected, the validator can reduce intra-batch aborts by selectively aborting transactions while choosing a good validation (and thus resulting serialization) order.
Such intra-batch transaction reordering can be done with several goals in mind. We can simply minimize intra-batch transaction aborts, i.e., we maximize the number of transactions in each batch that commit. Alternatively, we may also want to prioritize certain transactions to have a greater chance of committing. For example, if we want to reduce the transactions' tail latency, we can increase a transaction's priority every time it has to abort and restart. Priorities could also be related to external factors, e.g., a transaction's monetary value or an external, application-defined transaction priority. 
%These choices suggest a range of possible policies; we explore them in the next section.

%\subsection{Intra-Batch Validator Reordering (IBVR)}\label{sec:ibvr}
%\label{subsec:validator_reordering:algorithm}
We define the problem of intra-batch validator reordering (IBVR) more formally. A \emph{batch} $B$ is a set of transactions to be validated. We assume all transactions $t \in B$ are \emph{viable}, that is, no $t \in B$ conflicts with a previously committed transaction. If there are non-viable transactions in $B$, they can be removed in preprocessing, as they must always abort.
Given $B$, the goal of IBVR is to find a subset $B' \subseteq B$ of transactions to abort such that there is a serialization order $\prec$ for the 
the remaining transactions $B \setminus B'$, and all of the transactions in $B \setminus B'$ commit when validated in this order.
%that must abort due to intra-batch aborts. IBVR must also find a 
%strict (i.e. asymmetric) 
%total order $\prec$ on $B \setminus B'$ that \emph{respects all read-write dependencies}; that is, for $t,t'\in B \setminus B'$, if $t \prec t'$, then there is no read-write dependency from $t'$ to $t$.
We then processes each batch by running IBVR to identify $B'$ and $\prec$, aborting all the transactions in $B'$,  and validating the transaction in $B \setminus B'$  
in the order $\prec$. 
%By the constraint we gave on $\prec$ above, and from the discussion in Section~\ref{sec:background}, $\prec$ is guaranteed to be a valid serialization order that allows all transactions in $B \setminus B'$  to commit.
%Note that there is always a trivial solution to any IBVR instance: we can choose $B'$ to be all the elements of $B$ except one, so that $B \setminus B'$ has cardinality one. Such solutions are not useful; therefore, every instance of IBVR is associated with an \emph{objective function} on $B'$, and the goal is to find a $B'$ that maximizes the objective function. An example objective function could be the size of $B'$ (smaller is better), or a more complex function that takes into account transaction weights (e.g. priorities).
%Given $B$, the goal of IBVR is to find a $B' \subseteq B$ of transactions that must abort due to intra-batch aborts, and a commit order $Q$ for the remaining transactions that respects all read-write dependencies. The validator processes each batch by running IBVR to identify $B'$ and $Q$, aborting all the transactions in $B'$ and committing remaining transactions in the order of $Q$.
Note that there is always a trivial solution to any IBVR instance, namely to abort all transactions but one. 
This solution is not useful; therefore, every instance of
IBVR is associated with an \emph{objective function} on $B'$, and the goal is to
find a $B'$ that maximizes the objective function. A simple objective function
is the size of $B'$ (the fewer transactions to abort the better), and more complex functions can take transaction priorities
or the tail latency of transactions into
account. We call this objective function a \emph{policy} $P$.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\columnwidth]{./alg_fig/fvs-eg}
	%\vspace{-1em}
	\caption{An example of a directed graph; node 1 forms a feedback vertex set.}
	%\vspace{-1em}
	\label{fig:fvs}
\end{figure}

% dependency graph
%Our algorithms for IBVR are based on a graph representation of the transaction batches. 
How do we compute $B'$? We observe that every batch $B$ of viable transactions 
has an associated \emph{dependency graph} $G$, a directed graph whose nodes are the transactions in $B$ and whose edges are read-write dependencies.
If $G$ is acyclic, then there exists a commit order $Q$ on $G$ that respects all read-write dependencies. We can construct $Q$ by repeatedly committing a transaction whose corresponding node in $G$ has no outgoing edge using a topological sort. 

If $G$ is not acyclic, we can model this problem as an instance of the feedback vertex set problem. 
%show that the of finding a minimal $B'$ is NP-hard by reducing the NP-hard directed feedback vertex set problem 
%to the selection of a minimal $B'$~\cite{karp1972reducibility}. 
A feedback vertex set (FVS) of a directed graph is a subset of vertices whose removal makes the graph acyclic. For example, consider the graph in Figure~\ref{fig:fvs}. Vertex 1 forms a FVS since the graph becomes acyclic after removing Vertex 1 and its incoming and outgoing edges. Finding a minimal-size $B'$ for IBVR is exactly the problem of finding the minimal (smallest-size) feedback vertex set on $G$. If we have a more complex objective function for IBVR, we can assign weights to the nodes to represent the desired transaction priorities, and look for a minimum-weight FVS. 
Once we find the FVS $B'$, removing the nodes in $B'$ from $G$ yields an acyclic graph that determines the desired commit order $Q$.
The directed graph FVS (DFVS) problem is well-studied as it has many applications, including deadlock detection, program verification, and Bayesian inference. Unfortunately, it is NP-hard and APX-hard~\cite{kann1992approximability, karp1972reducibility}, and it is still an open problem whether there exists any constant-factor approximation.
We propose several practical algorithms for finding a FVS in the next section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  VALIDATOR BATCHING: OUR ALGORITHMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Validator Batching}
\label{sec:valbatching}
All our IBVR algorithms begin by constructing the dependency graph $G$. 
%We start with a batch of transactions and construct $B$ by discarding all non-viable transactions. We can identify such transactions by validating each transaction against all the updates prior to this batch.
We create one node per transaction, and one edge per read-write dependency. 
To determine whether a read-write dependency holds from transaction $t'$ to $t$, we check whether $WS(t) \cap RS(t') \neq \emptyset$. If so, we add an edge from $t'$ to $t$. We implement this by creating a hash table from the write sets and probing it with the read sets. Since a read in $t$ can potentially conflict with all the other transactions in the batch, the time complexity to probe the hash table for a single read is $O(|B|)$, where $|B|$ is the size of the batch. The complexity of building $G$ is thus $O(|B|^2+|R|+|W|)$, where $|R|$ is the total number of reads, and $|W|$ is the total number of writes. 

We now process $G$ to find a feedback vertex set. Both before and during the
execution of our FVS algorithms, we \emph{trim} the graph to remove all the nodes
that have no incoming edges and/or no outgoing edges since such nodes cannot
participate in any cycles.

\subsection{Algorithms}
\label{subsec:validator_reordering:algorithm}

\begin{algorithm}[t]
	\SetAlgoLined\DontPrintSemicolon
	\SetKwProg{main}{Algorithm}{}{}
	\main{GreedySccGraph($G$, $P$)}{
		\KwIn{Directed graph $G$, policy $P$}
		\KwOut{$V$, a feedback vertex set for $G$}
		$V\gets \emptyset$\;
		$G'\gets trim(G)$\;
		$SCC = StronglyConnectedComponents(G')$\;
		\For {$S \in SCC$} {
			$V\gets V\cup GreedyComponent(S, P)$\;
		}
		\Return{$V$}\;
	}{}
	
	\SetKwProg{sub}{Algorithm}{}{}
	\sub{GreedyComponent($S$, $P$)}{
		\KwIn{SCC $S$, policy $P$}
		\KwOut{$V'$, a feedback vertex set for $S$}
		
		\If {$S.size == 1$} {
			\Return{$\emptyset$}\;
		}
		$v\gets SelectVertexByPolicy(S, P)$\;
		$S'\gets GetGraphAfterVertexRemoval(S, v)$\;
		\Return{$v \cup GreedySccGraph(S', P)$}\;
	}{}
	
	\caption{SCC-based greedy algorithm}
	\label{alg:scc}
\end{algorithm}


\begin{algorithm}[t]
	\SetAlgoLined\DontPrintSemicolon
	\SetKwProg{main}{Algorithm}{}{}
	\main{GreedySortGraph($G$, $P$, $k$)}{
		\KwIn{Directed graph $G$, policy $P$, multi factor $k$}
		\KwOut{$V$, a feedback vertex set for $G$}
		$V\gets \emptyset$\;
		$G\gets trim(G)$\;
		\While {$G\neq \emptyset$} {
			\If {$G.size < k$} {
				$V\gets V\cup GreedySortGraph(G, P, 1)$\;
				break\;
			}
			$Q\gets QuickSelectVertexByPolicy(G, P, k)$\;
			\For {$i=1; i\leq k; ++i$} {
				$V\gets V\cup Q[i]$\;
				$G\gets GetGraphAfterVertexRemoval(G, Q[i])$\;
			}
			$G\gets trim(G)$\;
		}
		\Return{$V$}\;
	}{}
	\caption{Sort-based greedy algorithm}
	\label{alg:sort}
\end{algorithm}

{\bf SCC-Based Greedy Algorithm.} The intuition behind our first algorithm is that each cycle must be contained in a strongly connected component of the graph. After preprocessing, we partition the graph into SCCs. For a graph with $V$ vertices and $E$ edges, we can do this in time $O(|V|+|E|)$ using Tarjan's SCC algorithm~\cite{tarjan1972depth}.

Nodes in SCCs of size one cannot belong to any cycle. For a SCC that contains
more than one node, we choose a vertex to remove according to a \emph{policy}. The policy is a ranking function over vertices, and we greedily choose the top-ranked vertex to remove. We then recurse on the remaining graph. Algorithm~\ref{alg:scc} shows the details of this procedure. We begin by trimming and partitioning the graph into SCCs  (lines 3-4). We process each SCC $S$ using $GreedyComponent(S, P)$ (lines 5-7). This subroutine starts by eliminating SCCs of size one (lines 10-12). Next, it chooses the top-ranked vertex $v$ from $S$ under Policy $P$ (line 13). It removes $v$ from $S$ and recursively calls $GreedySccGraph$ on the remaining graph $S'$ (line 14 - 15). Finally, it returns the union of all the FVSs obtained in processing $S$ (line 15). When the top-level procedure $GreedySccGraph(G, P)$ has processed all the SCCs of $G$, it returns the union of the FVSs obtained (line 8).
% Time complexity analysis
Trimming and updating the graph after removing a node take $O(|V| + |E|)$ per
graph in total, since each node / edge can be removed only once. In the worst case, e.g., in a fully connected graph, we may only remove one node per iteration. Since SCC takes $O(|V|+|E|)$ per iteration, the time complexity of this algorithm is $O(|V|(|V|+|E|))$.
The policy $P$ is at the heart of the algorithm, and it ranks vertices which are likely to be included in a desirable FVS highly. We discuss possible policies in Section \ref{subsec:validator_reordering:policy}.

\begin{figure*}[t]
	\centering
	\begin{minipage}[b]{0.19\linewidth}
		\captionsetup{type=figure}
		\centering
		\subcaptionbox{original\label{fig:scc:g0}}
		{\includegraphics[width=\textwidth]{./alg_fig/scc-g0}}
		%     \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{partition into SCCs\label{fig:scc:g1}}
		{\includegraphics[width=\textwidth]{./alg_fig/scc-g1}}
		%    \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 3 to FVS\label{fig:scc:g3}}
		{\includegraphics[width=\textwidth]{./alg_fig/scc-g3}}
		%       \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 6 to FVS\label{fig:scc:g5}}
		{\includegraphics[width=\textwidth]{./alg_fig/scc-g5}}
		%      \vspace{-2em}
	\end{minipage}                  
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 8 to FVS\label{fig:scc:g7}}
		{\includegraphics[width=\textwidth]{./alg_fig/scc-g7}}
		%     \vspace{-2em}
	\end{minipage}  
	%\vspace{-1em}             
	\caption{An example of the SCC-based greedy algorithm using \texttt{prod-degree} policy. Blue nodes are trimmed during the algorithm, and yellow nodes form the FVS.}
	%\vspace{-1em}
	\label{fig:scc}
\end{figure*}

{\bf An example.} Figure~\ref{fig:scc} shows an instance of the SCC-based greedy algorithm that aims at minimizing the size of FVS, with a policy $P$ selecting the node with the largest product of its in-degree and out-degree, i.e., \texttt{prod-degree}. The graph cannot be trimmed, so we partition it into SCCs. We remove all SCCs of size 1 --  Nodes 0, 7, 10, 11, and 12 (Figure~\ref{fig:scc:g1}). There are three remaining SCCs. We first look at the component containing Nodes 3 and 4. Since Nodes 3 and 4 have the same product of in-degree and out-degree, we can add either one of them to the FVS. We choose Node 3. Now Node 4 has neither incoming nor outgoing edges, so it is trimmed (Figure~\ref{fig:scc:g3}). We repeat the process with the other components. For the SCC containing Nodes 2, 5, and 6, we add Node 6 to the FVS, as it has the largest product of in-degree and out-degree among the three nodes in this SCC. We now trim Nodes 2 and 5 (Figure~\ref{fig:scc:g5}). Finally, we remove Node 8 from the last component, and trim Nodes 1 and 9 (Figure~\ref{fig:scc:g7}). This leaves us with a final FVS consisting of Nodes 3, 6, and 8.

{\bf Sort-Based Greedy Algorithm.} Our first algorithm relies on a SCC partitioning routine that takes linear time in the size of the graph. As this routine is called several times throughout the algorithm, it can cause a non-trivial overhead. Here we propose a faster greedy algorithm using a sort-based approach to remove nodes. Through extensive empirical tests of the SCC-based greedy algorithm, we found that at each iteration, all the top ranked nodes in the graph are very likely to be included in the final FVS. Our second algorithm is based on this observation; it sorts the nodes according to a policy $P$, and includes the $k$ top-ranked nodes in the FVS. We call $k$ the \emph{multi factor} of the algorithm. The algorithm removes these nodes and iterates on the remaining graph.

Algorithm~\ref{alg:sort} shows this in more detail. We start by trimming the graph (line 3); if the graph has no more than $k$ nodes, we reduce the multi-factor to 1 (line 5-8). Otherwise, we sort and select top ranked $k$ nodes into a queue $Q$ using $P$ with the Quickselect algorithm~\cite{hoare61cacm}, and include the $k$ nodes in $V$ (line 9-13).
After removing the selected nodes from $G$, we trim the remaining the graph
again (line 14). We repeat this procedure until the graph is empty (line 4). 
% time complexity analysis.
As with the previous algorithm, updating the graph after removing a node and \emph{trim} takes $O(|V|+|E|)$ in total. Updating the weights of the remaining nodes takes $O(|V|)$ per iteration. The
Quickselect algorithm~\cite{hoare61cacm} has an amortized time complexity of
$O(|V|)$. In the worst case, it will take $O(|V|/k)$ iterations to terminate. So
the overall time complexity is $O(|V|^2/k + |V| + |E|)$.
This algorithm has smaller time complexity than the SCC-based greedy algorithm, and we can increase $k$ to further trade accuracy for shorter runtime. As we will see in Section~\ref{sec:experiments}, it produces results of comparable accuracy to the SCC-based algorithm in practice.

\begin{figure*}[t]
	\centering
	\begin{minipage}[b]{0.19\linewidth}
		\captionsetup{type=figure}
		\centering
		\subcaptionbox{original\label{fig:simple:g0}}
		{\includegraphics[width=\textwidth]{./alg_fig/simple-g0}}
		%     \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 5 to FVS\label{fig:simple:g2}}
		{\includegraphics[width=\textwidth]{./alg_fig/simple-g2}}
		%    \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 8 to FVS\label{fig:simple:g4}}
		{\includegraphics[width=\textwidth]{./alg_fig/simple-g4}}
		%       \vspace{-2em}
	\end{minipage}
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 6 to FVS\label{fig:simple:g6}}
		{\includegraphics[width=\textwidth]{./alg_fig/simple-g6}}
		%      \vspace{-2em}
	\end{minipage}                  
	\begin{minipage}[b]{0.19\linewidth}
		\centering
		\subcaptionbox{add 3 to FVS\label{fig:simple:g8}}
		{\includegraphics[width=\textwidth]{./alg_fig/simple-g8}}
		%     \vspace{-2em}
	\end{minipage}
	%\vspace{-1em}             
	\caption{An example of the sort-based greedy algorithm using \texttt{prod-degree} policy and multi factor 1. Blue nodes are trimmed during the algorithm, and yellow nodes form the FVS.}
	\label{fig:simple}               
	%\vspace{-1em}    	
\end{figure*}

{\bf Example (Continued).} Figure~\ref{fig:simple} shows the same example using the sort-based greedy algorithm, with $k = 1$. After the first sort, we add Node 5 to the FVS since it has the highest product of in-degree and out-degree. After eliminating Node 5, Nodes 10 and 12 have only incoming edges, and get trimmed (Figure~\ref{fig:simple:g2}). We sort the remaining nodes. This time, we add Node 8 to the FVS, and trim Nodes 0, 1, 9, 11 (Figure~\ref{fig:simple:g4}). We repeat this process with the remaining nodes until the graph is empty. This yields a FVS consisting of Nodes 3, 5, 6, and 8 (Figure~\ref{fig:simple:g8}), which contains one more vertex than the FVS we obtained with the SCC-based algorithm.

{\bf Hybrid Algorithm.}
%\subsubsection{Hybrid algorithm} 
We can combine the SCC-based greedy algorithm and the precise brute-force FVS search into a \emph{hybrid algorithm}. The algorithm is similar to the SCC-based greedy algorithm but runs a precise, brute-force FVS search whenever it can afford to. Instead of processing all SCCs via the $GreedyComponent$ subroutine (lines 5-7 of Algorithm~\ref{alg:scc}), it runs the precise search when processing SCCs that are smaller than a certain \emph{threshold}, and the greedy subroutine $GreedyComponent$ on SCCs that are larger than the threshold. Adjusting the threshold allows us to trade off precision versus runtime.

\subsection{Policies}
\label{subsec:validator_reordering:policy}
Policies are of utmost importance for our algorithms. Recall that a policy is a ranking function on vertices of the graph, and a good policy ranks vertices which are likely to be in a desirable FVS highly. 

% minimize number of conflicts
We first discuss policies that aim at minimizing the number of conflicts, i.e., the size of the FVS. The simplest such policy is \texttt{random} that assigns all nodes random rankings. Alternatively, we can rank nodes using degree-based heuristics, based on the intuition that the removal of a node will break many cycles if the node is high in some measurement of its graph degree. Such heuristics have  been shown to work well for FVS computation~\cite{cutello2015targeting}. For example, the policy \texttt{max-degree} chooses the node with the largest degree (either in-degree or out-degree), \texttt{sum-degree} chooses the node with the largest total degree (in-degree plus out-degree), and \texttt{prod-degree} chooses the node with the largest product of in-degree and out-degree. 
% minimize tail latency
More sophisticated policies are possible if the system is optimizing for a metric beyond maximizing the number of commits. For example, we may want to bound the transactions' tail latency; we can do that by incorporating latency information in our policies. We can rank transactions based on how many times they have been aborted and restarted; thus, transactions that have been restarted many times are much less likely to enter the FVS and have a higher chance of committing. Alternatively, we can also devise policies that combine the information about a transaction's number of restarts and its graph degree. For example, we can compute the ranking of a vertex as the product of its in-degree and out-degree divided by an exponential function of the number of restarts of the corresponding transaction. 
% help with transactions with monetary value
%\eat{We can also devise policies that handle hard transactions. In many workloads some transactions are naturally more prone to conflicts, e.g., those that access many objects and/or ``hot'' objects. If we use graph degree based policies, such transactions are likely to be included in the FVS and aborted repeatedly. To avoid starvation, we can adjust our policies to increase the probability that these transactions can commit. For example, we can approximate the ``hardness'' of a transaction by the size of its read and write sets and include that as a weighting factor in the policy.}
In business applications, the monetary value of different transactions varies. Optimizing for maximal monetary value, we can design policies that favor more valuable transactions. For example, we can customize the policy to always add a transaction with the lowest value to the FVS until the resulting dependency graph is acyclic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  PARALLELISM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Parallelism}
\label{sec:parallel}
\label{subsec:validator_reordering:parallel}
Since batching and reordering occur as part of the transaction execution, it can increase the overall transaction latency, 
resulting in a higher chance of conflicts. Thus, we introduce parallelism into validation wherever possible to reduce latency.
Recall that the validator first prepares a batch of transactions, then reorders them using one of our algorithms from Section \ref{sec:valbatching},  
and finally validates them and caches the resulting updates of transactions that commit.
 Each of these steps corresponds to a subcomponent in the validator. Parallelism across these components seems difficult since there are strict sequential dependencies among these components. In this section, we explain how we actually manage to parallelize execution within each component and then even across components.


Figure~\ref{fig:reorder:validator} shows the architecture of our parallel validator. The validator has three components: 
%through the use of pipelining, as well as within each component. 
The \emph{batch preparation component} receives transactions from the processor, packages them into batches, and sends batches to the \emph{transaction reordering component}. The transaction reordering component reorders the transactions using one of our algorithms from Section \ref{sec:valbatching}, and then sends a validation request to the \emph{transaction validation component}. The transaction validation component takes a batch of ordered 
transactions
and validates them against the latest validator cache. It also updates the validator cache with the updates from transactions that have passed the validation. 

{\bf Parallelism Within Each Component.}
First, we can introduce parallelism into each of the components. In batch preparation, multiple threads can package transactions into batches. We can either assign each processor to send its validation request to a specific batch preparation thread, or we can create a consumer-producer queue to connect the processors and the batch preparation threads. In transaction reordering, multiple threads can consume reordering requests from the batch preparation threads, and reorder batches of transactions concurrently. Since the batches are not ordered yet in the reordering stage (although transactions within each batch are now ordered), the threads can send the processed batches to the validation component in any order. At the validation component, the batches are processed in order and validated against all previously committed transactions. 
%We cannot simply validate each individual transaction on a different thread since transactions come from different batches can conflict with each other. 
Within an ordered batch, since the transactions are already serialized by the ordering produced from the reordering component, the only source of conflicts is from all the transactions committed prior to the batch. Thus the validation of all transactions within a batch can be processed in parallel. Since conflicts can happen across batch boundaries, processing a new batch can only start after all the transactions in the previous batch have been validated. The updates from committed transactions in a batch are applied to the validator cache in serialization order at the end of the processing of the batch. Alternatively, we can use partitioned parallelism within a batch: We partition the key space and break a transaction into smaller pieces, one for each piece of the key space, and we assign different threads to validate transaction pieces in different partitions. This is analogous to the design of a partitioned validator in~\cite{ding2015centiman}.


% Pipeline parallelism
{\bf Parallelism Across Components.}
We can further increase parallelism by running the three validator components in parallel using pipelined parallelism, 
with each component working on a different batch in parallel. The batches then shift from component to component through the validator.

{\bf A Further Refinement: Pre-Validation.} As we mentioned in Section~\ref{subsec:validator_reordering}, prior to reordering, we can remove transactions that are not viable, i.e., conflicting with previously committed transactions. While this \emph{pre-validation} adds an additional validation for transactions on top of the final validation which serializes them, it reduces the number of transactions to reorder and the reordering algorithm runs faster. In our experiments, we observed that the reordering component was by a large factor the bottleneck piece, and eliminating non-viable transactions with pre-validation significantly improved performance. Thus, after batch creation, we first pre-validate the batch, reorder the remaining transactions, and then perform a second and final validation against the current database state.

We illustrate this design in Figure \ref{fig:reorder:validator}. The bottom row shows the set of database snapshots, one after each batch of transactions that has been validated. Consider a batch $B$. With pre-validation, $B$ will get validated against a ``stale'' database state $S$. Those transactions within batch $B$ that did not abort during pre-validation were then re-ordered and validated a second time in the transaction validation component against the ``correct'' database state $S'$. Since $S'$ now reflects all the updates from transactions that committed while $B$ was in the reordering component, transactions in $B$ can show conflicts during the final validation even through they were once already pre-validated in the reordering component.

%. We denote a database snapshot as $S_k$, where transactions whose timestamps are less or equal to $k$ have been committed as of this snapshot. In the reordering subcomponent, a batch of transactions $B$ is validated against the latest database snapshot $S_m$. The remaining transactions after pre-filtering are reordered as a batch $B'$ and sent to the validation\eat{ subcomponent}. $B'$ is again validated against the current database snapshot $S_n$. Since the validation subcomponent can be validating uncommitted transactions while $B$ is being processed at the reordering subcomponent, when $B'$ arrives at the validation\eat{ subcomponent},  we may have $n\geq m$. Thus, transactions in $B'$ can still abort due to conflicts with previously committed transactions in $S_n$.

% Parallelism in each subcomponent.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{./figures/validator}
	\vspace{-2em}
	\caption{The architecture of parallel validator. It is decoupled into three subcomponents for pipeline parallelism: batch preparation, transaction reordering, and transaction validation. Each subcomponent can be further parallelized.}
	\vspace{-1em}
	\label{fig:reorder:validator}
\end{figure}
